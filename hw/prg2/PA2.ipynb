{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 2\n",
    "Drew Rosales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import graphviz\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "### 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_df = pd.read_csv(\"mnist_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0          5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1          0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2          4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3          1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4          9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
       "59995      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59996      3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59997      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59998      6    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59999      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "       28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0          0      0      0      0      0      0      0      0  \n",
       "1          0      0      0      0      0      0      0      0  \n",
       "2          0      0      0      0      0      0      0      0  \n",
       "3          0      0      0      0      0      0      0      0  \n",
       "4          0      0      0      0      0      0      0      0  \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "59995      0      0      0      0      0      0      0      0  \n",
       "59996      0      0      0      0      0      0      0      0  \n",
       "59997      0      0      0      0      0      0      0      0  \n",
       "59998      0      0      0      0      0      0      0      0  \n",
       "59999      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[60000 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "#citation: inspired by DataProcessing.pdf class notes\n",
    "def fisher_two_hot(df, classes):\n",
    "    x1 = df[df[\"label\"] == classes[0]].iloc[:, 1:785].values\n",
    "    x2 = df[df[\"label\"] == classes[1]].iloc[:, 1:785].values\n",
    "\n",
    "    mu1 = np.mean(x1, axis=0)\n",
    "    mu2 = np.mean(x2, axis=0)\n",
    "\n",
    "    sigma1 = np.std(x1, axis=0)**2\n",
    "    sigma2 = np.std(x2, axis=0)**2\n",
    "\n",
    "    fdr = (mu1 - mu2)**2 / (sigma1 + sigma2)\n",
    "    return fdr\n",
    "\n",
    "def fisher_discriminant(df, labels):\n",
    "    x = df.iloc[:,1:785]\n",
    "    ranking = {} \n",
    "    classes = np.unique(labels)\n",
    "    comb = list(combinations(classes, 2))\n",
    "    print(comb)\n",
    "    fdr_arr = np.zeros((len(comb), x.shape[1]))\n",
    "\n",
    "    for idx, i in enumerate(comb):\n",
    "        #list of class names pairs\n",
    "        c = list(i)\n",
    "        fdr_arr[idx, :]  = fisher_two_hot(df, c)\n",
    "\n",
    "    \n",
    "    fdr = np.sum(fdr_arr, axis=0)\n",
    "\n",
    "    #print(fdr)\n",
    "\n",
    "    for idx, x in enumerate(np.argsort(fdr)[::-1]):\n",
    "        ranking[f\"feature_{x+1}\"] = idx+1\n",
    "    \n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(np.int64(0), np.int64(1)), (np.int64(0), np.int64(2)), (np.int64(0), np.int64(3)), (np.int64(0), np.int64(4)), (np.int64(0), np.int64(5)), (np.int64(0), np.int64(6)), (np.int64(0), np.int64(7)), (np.int64(0), np.int64(8)), (np.int64(0), np.int64(9)), (np.int64(1), np.int64(2)), (np.int64(1), np.int64(3)), (np.int64(1), np.int64(4)), (np.int64(1), np.int64(5)), (np.int64(1), np.int64(6)), (np.int64(1), np.int64(7)), (np.int64(1), np.int64(8)), (np.int64(1), np.int64(9)), (np.int64(2), np.int64(3)), (np.int64(2), np.int64(4)), (np.int64(2), np.int64(5)), (np.int64(2), np.int64(6)), (np.int64(2), np.int64(7)), (np.int64(2), np.int64(8)), (np.int64(2), np.int64(9)), (np.int64(3), np.int64(4)), (np.int64(3), np.int64(5)), (np.int64(3), np.int64(6)), (np.int64(3), np.int64(7)), (np.int64(3), np.int64(8)), (np.int64(3), np.int64(9)), (np.int64(4), np.int64(5)), (np.int64(4), np.int64(6)), (np.int64(4), np.int64(7)), (np.int64(4), np.int64(8)), (np.int64(4), np.int64(9)), (np.int64(5), np.int64(6)), (np.int64(5), np.int64(7)), (np.int64(5), np.int64(8)), (np.int64(5), np.int64(9)), (np.int64(6), np.int64(7)), (np.int64(6), np.int64(8)), (np.int64(6), np.int64(9)), (np.int64(7), np.int64(8)), (np.int64(7), np.int64(9)), (np.int64(8), np.int64(9))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8305/559208818.py:14: RuntimeWarning: invalid value encountered in divide\n",
      "  fdr = (mu1 - mu2)**2 / (sigma1 + sigma2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'feature_168': 1,\n",
       " 'feature_169': 2,\n",
       " 'feature_170': 3,\n",
       " 'feature_171': 4,\n",
       " 'feature_172': 5,\n",
       " 'feature_195': 6,\n",
       " 'feature_196': 7,\n",
       " 'feature_197': 8,\n",
       " 'feature_198': 9,\n",
       " 'feature_199': 10,\n",
       " 'feature_223': 11,\n",
       " 'feature_562': 12,\n",
       " 'feature_225': 13,\n",
       " 'feature_226': 14,\n",
       " 'feature_227': 15,\n",
       " 'feature_228': 16,\n",
       " 'feature_250': 17,\n",
       " 'feature_251': 18,\n",
       " 'feature_252': 19,\n",
       " 'feature_253': 20,\n",
       " 'feature_254': 21,\n",
       " 'feature_255': 22,\n",
       " 'feature_279': 23,\n",
       " 'feature_280': 24,\n",
       " 'feature_224': 25,\n",
       " 'feature_105': 26,\n",
       " 'feature_106': 27,\n",
       " 'feature_107': 28,\n",
       " 'feature_108': 29,\n",
       " 'feature_109': 30,\n",
       " 'feature_110': 31,\n",
       " 'feature_111': 32,\n",
       " 'feature_112': 33,\n",
       " 'feature_113': 34,\n",
       " 'feature_114': 35,\n",
       " 'feature_115': 36,\n",
       " 'feature_167': 37,\n",
       " 'feature_117': 38,\n",
       " 'feature_136': 39,\n",
       " 'feature_137': 40,\n",
       " 'feature_138': 41,\n",
       " 'feature_139': 42,\n",
       " 'feature_140': 43,\n",
       " 'feature_141': 44,\n",
       " 'feature_142': 45,\n",
       " 'feature_143': 46,\n",
       " 'feature_144': 47,\n",
       " 'feature_165': 48,\n",
       " 'feature_166': 49,\n",
       " 'feature_116': 50,\n",
       " 'feature_423': 51,\n",
       " 'feature_447': 52,\n",
       " 'feature_448': 53,\n",
       " 'feature_449': 54,\n",
       " 'feature_450': 55,\n",
       " 'feature_451': 56,\n",
       " 'feature_475': 57,\n",
       " 'feature_476': 58,\n",
       " 'feature_477': 59,\n",
       " 'feature_478': 60,\n",
       " 'feature_479': 61,\n",
       " 'feature_281': 62,\n",
       " 'feature_504': 63,\n",
       " 'feature_505': 64,\n",
       " 'feature_506': 65,\n",
       " 'feature_507': 66,\n",
       " 'feature_531': 67,\n",
       " 'feature_532': 68,\n",
       " 'feature_533': 69,\n",
       " 'feature_534': 70,\n",
       " 'feature_535': 71,\n",
       " 'feature_559': 72,\n",
       " 'feature_560': 73,\n",
       " 'feature_561': 74,\n",
       " 'feature_503': 75,\n",
       " 'feature_282': 76,\n",
       " 'feature_283': 77,\n",
       " 'feature_284': 78,\n",
       " 'feature_307': 79,\n",
       " 'feature_308': 80,\n",
       " 'feature_309': 81,\n",
       " 'feature_310': 82,\n",
       " 'feature_311': 83,\n",
       " 'feature_336': 84,\n",
       " 'feature_337': 85,\n",
       " 'feature_338': 86,\n",
       " 'feature_422': 87,\n",
       " 'feature_363': 88,\n",
       " 'feature_364': 89,\n",
       " 'feature_365': 90,\n",
       " 'feature_366': 91,\n",
       " 'feature_367': 92,\n",
       " 'feature_392': 93,\n",
       " 'feature_1': 94,\n",
       " 'feature_394': 95,\n",
       " 'feature_395': 96,\n",
       " 'feature_419': 97,\n",
       " 'feature_420': 98,\n",
       " 'feature_421': 99,\n",
       " 'feature_339': 100,\n",
       " 'feature_27': 101,\n",
       " 'feature_28': 102,\n",
       " 'feature_29': 103,\n",
       " 'feature_30': 104,\n",
       " 'feature_31': 105,\n",
       " 'feature_32': 106,\n",
       " 'feature_33': 107,\n",
       " 'feature_34': 108,\n",
       " 'feature_35': 109,\n",
       " 'feature_36': 110,\n",
       " 'feature_37': 111,\n",
       " 'feature_101': 112,\n",
       " 'feature_39': 113,\n",
       " 'feature_40': 114,\n",
       " 'feature_41': 115,\n",
       " 'feature_42': 116,\n",
       " 'feature_43': 117,\n",
       " 'feature_44': 118,\n",
       " 'feature_45': 119,\n",
       " 'feature_46': 120,\n",
       " 'feature_47': 121,\n",
       " 'feature_48': 122,\n",
       " 'feature_49': 123,\n",
       " 'feature_50': 124,\n",
       " 'feature_38': 125,\n",
       " 'feature_2': 126,\n",
       " 'feature_3': 127,\n",
       " 'feature_4': 128,\n",
       " 'feature_5': 129,\n",
       " 'feature_6': 130,\n",
       " 'feature_7': 131,\n",
       " 'feature_8': 132,\n",
       " 'feature_9': 133,\n",
       " 'feature_10': 134,\n",
       " 'feature_11': 135,\n",
       " 'feature_12': 136,\n",
       " 'feature_26': 137,\n",
       " 'feature_14': 138,\n",
       " 'feature_15': 139,\n",
       " 'feature_16': 140,\n",
       " 'feature_17': 141,\n",
       " 'feature_18': 142,\n",
       " 'feature_19': 143,\n",
       " 'feature_20': 144,\n",
       " 'feature_21': 145,\n",
       " 'feature_22': 146,\n",
       " 'feature_23': 147,\n",
       " 'feature_24': 148,\n",
       " 'feature_25': 149,\n",
       " 'feature_13': 150,\n",
       " 'feature_77': 151,\n",
       " 'feature_78': 152,\n",
       " 'feature_79': 153,\n",
       " 'feature_80': 154,\n",
       " 'feature_81': 155,\n",
       " 'feature_82': 156,\n",
       " 'feature_83': 157,\n",
       " 'feature_84': 158,\n",
       " 'feature_85': 159,\n",
       " 'feature_86': 160,\n",
       " 'feature_87': 161,\n",
       " 'feature_51': 162,\n",
       " 'feature_89': 163,\n",
       " 'feature_90': 164,\n",
       " 'feature_91': 165,\n",
       " 'feature_92': 166,\n",
       " 'feature_93': 167,\n",
       " 'feature_94': 168,\n",
       " 'feature_95': 169,\n",
       " 'feature_96': 170,\n",
       " 'feature_97': 171,\n",
       " 'feature_98': 172,\n",
       " 'feature_99': 173,\n",
       " 'feature_100': 174,\n",
       " 'feature_88': 175,\n",
       " 'feature_52': 176,\n",
       " 'feature_53': 177,\n",
       " 'feature_54': 178,\n",
       " 'feature_55': 179,\n",
       " 'feature_56': 180,\n",
       " 'feature_57': 181,\n",
       " 'feature_58': 182,\n",
       " 'feature_59': 183,\n",
       " 'feature_60': 184,\n",
       " 'feature_61': 185,\n",
       " 'feature_62': 186,\n",
       " 'feature_76': 187,\n",
       " 'feature_64': 188,\n",
       " 'feature_65': 189,\n",
       " 'feature_66': 190,\n",
       " 'feature_67': 191,\n",
       " 'feature_68': 192,\n",
       " 'feature_69': 193,\n",
       " 'feature_70': 194,\n",
       " 'feature_71': 195,\n",
       " 'feature_72': 196,\n",
       " 'feature_73': 197,\n",
       " 'feature_74': 198,\n",
       " 'feature_75': 199,\n",
       " 'feature_63': 200,\n",
       " 'feature_746': 201,\n",
       " 'feature_747': 202,\n",
       " 'feature_748': 203,\n",
       " 'feature_749': 204,\n",
       " 'feature_750': 205,\n",
       " 'feature_705': 206,\n",
       " 'feature_704': 207,\n",
       " 'feature_703': 208,\n",
       " 'feature_702': 209,\n",
       " 'feature_701': 210,\n",
       " 'feature_700': 211,\n",
       " 'feature_699': 212,\n",
       " 'feature_736': 213,\n",
       " 'feature_697': 214,\n",
       " 'feature_751': 215,\n",
       " 'feature_752': 216,\n",
       " 'feature_753': 217,\n",
       " 'feature_677': 218,\n",
       " 'feature_676': 219,\n",
       " 'feature_675': 220,\n",
       " 'feature_754': 221,\n",
       " 'feature_755': 222,\n",
       " 'feature_756': 223,\n",
       " 'feature_757': 224,\n",
       " 'feature_758': 225,\n",
       " 'feature_698': 226,\n",
       " 'feature_735': 227,\n",
       " 'feature_734': 228,\n",
       " 'feature_737': 229,\n",
       " 'feature_738': 230,\n",
       " 'feature_739': 231,\n",
       " 'feature_733': 232,\n",
       " 'feature_732': 233,\n",
       " 'feature_731': 234,\n",
       " 'feature_730': 235,\n",
       " 'feature_729': 236,\n",
       " 'feature_728': 237,\n",
       " 'feature_727': 238,\n",
       " 'feature_706': 239,\n",
       " 'feature_725': 240,\n",
       " 'feature_724': 241,\n",
       " 'feature_723': 242,\n",
       " 'feature_740': 243,\n",
       " 'feature_741': 244,\n",
       " 'feature_722': 245,\n",
       " 'feature_721': 246,\n",
       " 'feature_720': 247,\n",
       " 'feature_742': 248,\n",
       " 'feature_743': 249,\n",
       " 'feature_744': 250,\n",
       " 'feature_745': 251,\n",
       " 'feature_719': 252,\n",
       " 'feature_726': 253,\n",
       " 'feature_588': 254,\n",
       " 'feature_587': 255,\n",
       " 'feature_771': 256,\n",
       " 'feature_772': 257,\n",
       " 'feature_773': 258,\n",
       " 'feature_774': 259,\n",
       " 'feature_775': 260,\n",
       " 'feature_776': 261,\n",
       " 'feature_621': 262,\n",
       " 'feature_649': 263,\n",
       " 'feature_648': 264,\n",
       " 'feature_647': 265,\n",
       " 'feature_759': 266,\n",
       " 'feature_645': 267,\n",
       " 'feature_777': 268,\n",
       " 'feature_778': 269,\n",
       " 'feature_779': 270,\n",
       " 'feature_780': 271,\n",
       " 'feature_644': 272,\n",
       " 'feature_643': 273,\n",
       " 'feature_393': 274,\n",
       " 'feature_781': 275,\n",
       " 'feature_782': 276,\n",
       " 'feature_783': 277,\n",
       " 'feature_784': 278,\n",
       " 'feature_646': 279,\n",
       " 'feature_592': 280,\n",
       " 'feature_591': 281,\n",
       " 'feature_590': 282,\n",
       " 'feature_589': 283,\n",
       " 'feature_760': 284,\n",
       " 'feature_761': 285,\n",
       " 'feature_762': 286,\n",
       " 'feature_674': 287,\n",
       " 'feature_673': 288,\n",
       " 'feature_672': 289,\n",
       " 'feature_615': 290,\n",
       " 'feature_616': 291,\n",
       " 'feature_770': 292,\n",
       " 'feature_671': 293,\n",
       " 'feature_763': 294,\n",
       " 'feature_764': 295,\n",
       " 'feature_765': 296,\n",
       " 'feature_618': 297,\n",
       " 'feature_619': 298,\n",
       " 'feature_620': 299,\n",
       " 'feature_766': 300,\n",
       " 'feature_767': 301,\n",
       " 'feature_768': 302,\n",
       " 'feature_769': 303,\n",
       " 'feature_617': 304,\n",
       " 'feature_407': 305,\n",
       " 'feature_435': 306,\n",
       " 'feature_379': 307,\n",
       " 'feature_351': 308,\n",
       " 'feature_434': 309,\n",
       " 'feature_463': 310,\n",
       " 'feature_462': 311,\n",
       " 'feature_410': 312,\n",
       " 'feature_406': 313,\n",
       " 'feature_438': 314,\n",
       " 'feature_490': 315,\n",
       " 'feature_437': 316,\n",
       " 'feature_352': 317,\n",
       " 'feature_380': 318,\n",
       " 'feature_378': 319,\n",
       " 'feature_543': 320,\n",
       " 'feature_382': 321,\n",
       " 'feature_598': 322,\n",
       " 'feature_569': 323,\n",
       " 'feature_408': 324,\n",
       " 'feature_409': 325,\n",
       " 'feature_570': 326,\n",
       " 'feature_544': 327,\n",
       " 'feature_465': 328,\n",
       " 'feature_597': 329,\n",
       " 'feature_515': 330,\n",
       " 'feature_402': 331,\n",
       " 'feature_374': 332,\n",
       " 'feature_156': 333,\n",
       " 'feature_430': 334,\n",
       " 'feature_436': 335,\n",
       " 'feature_516': 336,\n",
       " 'feature_324': 337,\n",
       " 'feature_429': 338,\n",
       " 'feature_401': 339,\n",
       " 'feature_461': 340,\n",
       " 'feature_346': 341,\n",
       " 'feature_489': 342,\n",
       " 'feature_542': 343,\n",
       " 'feature_571': 344,\n",
       " 'feature_155': 345,\n",
       " 'feature_347': 346,\n",
       " 'feature_541': 347,\n",
       " 'feature_491': 348,\n",
       " 'feature_319': 349,\n",
       " 'feature_375': 350,\n",
       " 'feature_157': 351,\n",
       " 'feature_291': 352,\n",
       " 'feature_464': 353,\n",
       " 'feature_568': 354,\n",
       " 'feature_518': 355,\n",
       " 'feature_626': 356,\n",
       " 'feature_377': 357,\n",
       " 'feature_373': 358,\n",
       " 'feature_376': 359,\n",
       " 'feature_540': 360,\n",
       " 'feature_517': 361,\n",
       " 'feature_318': 362,\n",
       " 'feature_457': 363,\n",
       " 'feature_431': 364,\n",
       " 'feature_488': 365,\n",
       " 'feature_657': 366,\n",
       " 'feature_627': 367,\n",
       " 'feature_184': 368,\n",
       " 'feature_403': 369,\n",
       " 'feature_264': 370,\n",
       " 'feature_599': 371,\n",
       " 'feature_458': 372,\n",
       " 'feature_348': 373,\n",
       " 'feature_355': 374,\n",
       " 'feature_433': 375,\n",
       " 'feature_514': 376,\n",
       " 'feature_572': 377,\n",
       " 'feature_323': 378,\n",
       " 'feature_487': 379,\n",
       " 'feature_466': 380,\n",
       " 'feature_292': 381,\n",
       " 'feature_154': 382,\n",
       " 'feature_383': 383,\n",
       " 'feature_512': 384,\n",
       " 'feature_381': 385,\n",
       " 'feature_658': 386,\n",
       " 'feature_327': 387,\n",
       " 'feature_456': 388,\n",
       " 'feature_459': 389,\n",
       " 'feature_513': 390,\n",
       " 'feature_545': 391,\n",
       " 'feature_486': 392,\n",
       " 'feature_428': 393,\n",
       " 'feature_354': 394,\n",
       " 'feature_185': 395,\n",
       " 'feature_183': 396,\n",
       " 'feature_350': 397,\n",
       " 'feature_484': 398,\n",
       " 'feature_656': 399,\n",
       " 'feature_524': 400,\n",
       " 'feature_405': 401,\n",
       " 'feature_485': 402,\n",
       " 'feature_411': 403,\n",
       " 'feature_596': 404,\n",
       " 'feature_460': 405,\n",
       " 'feature_320': 406,\n",
       " 'feature_551': 407,\n",
       " 'feature_404': 408,\n",
       " 'feature_263': 409,\n",
       " 'feature_345': 410,\n",
       " 'feature_625': 411,\n",
       " 'feature_158': 412,\n",
       " 'feature_432': 413,\n",
       " 'feature_349': 414,\n",
       " 'feature_400': 415,\n",
       " 'feature_628': 416,\n",
       " 'feature_497': 417,\n",
       " 'feature_299': 418,\n",
       " 'feature_552': 419,\n",
       " 'feature_290': 420,\n",
       " 'feature_492': 421,\n",
       " 'feature_496': 422,\n",
       " 'feature_300': 423,\n",
       " 'feature_153': 424,\n",
       " 'feature_469': 425,\n",
       " 'feature_265': 426,\n",
       " 'feature_296': 427,\n",
       " 'feature_328': 428,\n",
       " 'feature_182': 429,\n",
       " 'feature_439': 430,\n",
       " 'feature_186': 431,\n",
       " 'feature_493': 432,\n",
       " 'feature_414': 433,\n",
       " 'feature_523': 434,\n",
       " 'feature_271': 435,\n",
       " 'feature_386': 436,\n",
       " 'feature_655': 437,\n",
       " 'feature_272': 438,\n",
       " 'feature_525': 439,\n",
       " 'feature_442': 440,\n",
       " 'feature_573': 441,\n",
       " 'feature_358': 442,\n",
       " 'feature_659': 443,\n",
       " 'feature_330': 444,\n",
       " 'feature_539': 445,\n",
       " 'feature_301': 446,\n",
       " 'feature_273': 447,\n",
       " 'feature_579': 448,\n",
       " 'feature_441': 449,\n",
       " 'feature_550': 450,\n",
       " 'feature_270': 451,\n",
       " 'feature_519': 452,\n",
       " 'feature_244': 453,\n",
       " 'feature_578': 454,\n",
       " 'feature_181': 455,\n",
       " 'feature_326': 456,\n",
       " 'feature_567': 457,\n",
       " 'feature_546': 458,\n",
       " 'feature_353': 459,\n",
       " 'feature_470': 460,\n",
       " 'feature_317': 461,\n",
       " 'feature_511': 462,\n",
       " 'feature_372': 463,\n",
       " 'feature_329': 464,\n",
       " 'feature_243': 465,\n",
       " 'feature_236': 466,\n",
       " 'feature_237': 467,\n",
       " 'feature_302': 468,\n",
       " 'feature_128': 469,\n",
       " 'feature_180': 470,\n",
       " 'feature_468': 471,\n",
       " 'feature_152': 472,\n",
       " 'feature_629': 473,\n",
       " 'feature_298': 474,\n",
       " 'feature_387': 475,\n",
       " 'feature_483': 476,\n",
       " 'feature_187': 477,\n",
       " 'feature_245': 478,\n",
       " 'feature_356': 479,\n",
       " 'feature_359': 480,\n",
       " 'feature_600': 481,\n",
       " 'feature_127': 482,\n",
       " 'feature_212': 483,\n",
       " 'feature_553': 484,\n",
       " 'feature_415': 485,\n",
       " 'feature_129': 486,\n",
       " 'feature_455': 487,\n",
       " 'feature_269': 488,\n",
       " 'feature_268': 489,\n",
       " 'feature_624': 490,\n",
       " 'feature_467': 491,\n",
       " 'feature_262': 492,\n",
       " 'feature_498': 493,\n",
       " 'feature_413': 494,\n",
       " 'feature_159': 495,\n",
       " 'feature_242': 496,\n",
       " 'feature_654': 497,\n",
       " 'feature_325': 498,\n",
       " 'feature_274': 499,\n",
       " 'feature_440': 500,\n",
       " 'feature_580': 501,\n",
       " 'feature_293': 502,\n",
       " 'feature_216': 503,\n",
       " 'feature_297': 504,\n",
       " 'feature_215': 505,\n",
       " 'feature_494': 506,\n",
       " 'feature_495': 507,\n",
       " 'feature_595': 508,\n",
       " 'feature_357': 509,\n",
       " 'feature_630': 510,\n",
       " 'feature_179': 511,\n",
       " 'feature_321': 512,\n",
       " 'feature_427': 513,\n",
       " 'feature_331': 514,\n",
       " 'feature_213': 515,\n",
       " 'feature_211': 516,\n",
       " 'feature_240': 517,\n",
       " 'feature_126': 518,\n",
       " 'feature_443': 519,\n",
       " 'feature_685': 520,\n",
       " 'feature_238': 521,\n",
       " 'feature_241': 522,\n",
       " 'feature_660': 523,\n",
       " 'feature_188': 524,\n",
       " 'feature_385': 525,\n",
       " 'feature_412': 526,\n",
       " 'feature_235': 527,\n",
       " 'feature_344': 528,\n",
       " 'feature_574': 529,\n",
       " 'feature_631': 530,\n",
       " 'feature_214': 531,\n",
       " 'feature_686': 532,\n",
       " 'feature_526': 533,\n",
       " 'feature_384': 534,\n",
       " 'feature_289': 535,\n",
       " 'feature_239': 536,\n",
       " 'feature_522': 537,\n",
       " 'feature_217': 538,\n",
       " 'feature_130': 539,\n",
       " 'feature_606': 540,\n",
       " 'feature_151': 541,\n",
       " 'feature_246': 542,\n",
       " 'feature_295': 543,\n",
       " 'feature_266': 544,\n",
       " 'feature_577': 545,\n",
       " 'feature_684': 546,\n",
       " 'feature_520': 547,\n",
       " 'feature_267': 548,\n",
       " 'feature_632': 549,\n",
       " 'feature_399': 550,\n",
       " 'feature_303': 551,\n",
       " 'feature_581': 552,\n",
       " 'feature_607': 553,\n",
       " 'feature_471': 554,\n",
       " 'feature_322': 555,\n",
       " 'feature_125': 556,\n",
       " 'feature_178': 557,\n",
       " 'feature_210': 558,\n",
       " 'feature_189': 559,\n",
       " 'feature_554': 560,\n",
       " 'feature_687': 561,\n",
       " 'feature_208': 562,\n",
       " 'feature_653': 563,\n",
       " 'feature_605': 564,\n",
       " 'feature_547': 565,\n",
       " 'feature_209': 566,\n",
       " 'feature_218': 567,\n",
       " 'feature_160': 568,\n",
       " 'feature_633': 569,\n",
       " 'feature_538': 570,\n",
       " 'feature_713': 571,\n",
       " 'feature_521': 572,\n",
       " 'feature_275': 573,\n",
       " 'feature_207': 574,\n",
       " 'feature_261': 575,\n",
       " 'feature_316': 576,\n",
       " 'feature_510': 577,\n",
       " 'feature_234': 578,\n",
       " 'feature_601': 579,\n",
       " 'feature_566': 580,\n",
       " 'feature_714': 581,\n",
       " 'feature_683': 582,\n",
       " 'feature_623': 583,\n",
       " 'feature_499': 584,\n",
       " 'feature_712': 585,\n",
       " 'feature_661': 586,\n",
       " 'feature_549': 587,\n",
       " 'feature_247': 588,\n",
       " 'feature_608': 589,\n",
       " 'feature_388': 590,\n",
       " 'feature_131': 591,\n",
       " 'feature_190': 592,\n",
       " 'feature_371': 593,\n",
       " 'feature_150': 594,\n",
       " 'feature_206': 595,\n",
       " 'feature_634': 596,\n",
       " 'feature_482': 597,\n",
       " 'feature_688': 598,\n",
       " 'feature_124': 599,\n",
       " 'feature_360': 600,\n",
       " 'feature_219': 601,\n",
       " 'feature_575': 602,\n",
       " 'feature_416': 603,\n",
       " 'feature_102': 604,\n",
       " 'feature_582': 605,\n",
       " 'feature_715': 606,\n",
       " 'feature_177': 607,\n",
       " 'feature_527': 608,\n",
       " 'feature_454': 609,\n",
       " 'feature_604': 610,\n",
       " 'feature_594': 611,\n",
       " 'feature_233': 612,\n",
       " 'feature_288': 613,\n",
       " 'feature_205': 614,\n",
       " 'feature_444': 615,\n",
       " 'feature_332': 616,\n",
       " 'feature_711': 617,\n",
       " 'feature_576': 618,\n",
       " 'feature_191': 619,\n",
       " 'feature_294': 620,\n",
       " 'feature_161': 621,\n",
       " 'feature_602': 622,\n",
       " 'feature_555': 623,\n",
       " 'feature_609': 624,\n",
       " 'feature_652': 625,\n",
       " 'feature_635': 626,\n",
       " 'feature_220': 627,\n",
       " 'feature_689': 628,\n",
       " 'feature_260': 629,\n",
       " 'feature_716': 630,\n",
       " 'feature_662': 631,\n",
       " 'feature_248': 632,\n",
       " 'feature_103': 633,\n",
       " 'feature_682': 634,\n",
       " 'feature_426': 635,\n",
       " 'feature_343': 636,\n",
       " 'feature_603': 637,\n",
       " 'feature_232': 638,\n",
       " 'feature_304': 639,\n",
       " 'feature_548': 640,\n",
       " 'feature_123': 641,\n",
       " 'feature_276': 642,\n",
       " 'feature_472': 643,\n",
       " 'feature_204': 644,\n",
       " 'feature_192': 645,\n",
       " 'feature_132': 646,\n",
       " 'feature_149': 647,\n",
       " 'feature_583': 648,\n",
       " 'feature_690': 649,\n",
       " 'feature_717': 650,\n",
       " 'feature_162': 651,\n",
       " 'feature_610': 652,\n",
       " 'feature_176': 653,\n",
       " 'feature_636': 654,\n",
       " 'feature_398': 655,\n",
       " 'feature_710': 656,\n",
       " 'feature_259': 657,\n",
       " 'feature_231': 658,\n",
       " 'feature_500': 659,\n",
       " 'feature_315': 660,\n",
       " 'feature_622': 661,\n",
       " 'feature_663': 662,\n",
       " 'feature_528': 663,\n",
       " 'feature_221': 664,\n",
       " 'feature_287': 665,\n",
       " 'feature_104': 666,\n",
       " 'feature_163': 667,\n",
       " 'feature_249': 668,\n",
       " 'feature_556': 669,\n",
       " 'feature_565': 670,\n",
       " 'feature_718': 671,\n",
       " 'feature_537': 672,\n",
       " 'feature_203': 673,\n",
       " 'feature_681': 674,\n",
       " 'feature_122': 675,\n",
       " 'feature_691': 676,\n",
       " 'feature_651': 677,\n",
       " 'feature_637': 678,\n",
       " 'feature_370': 679,\n",
       " 'feature_193': 680,\n",
       " 'feature_258': 681,\n",
       " 'feature_611': 682,\n",
       " 'feature_593': 683,\n",
       " 'feature_509': 684,\n",
       " 'feature_584': 685,\n",
       " 'feature_164': 686,\n",
       " 'feature_148': 687,\n",
       " 'feature_230': 688,\n",
       " 'feature_133': 689,\n",
       " 'feature_277': 690,\n",
       " 'feature_664': 691,\n",
       " 'feature_286': 692,\n",
       " 'feature_417': 693,\n",
       " 'feature_709': 694,\n",
       " 'feature_175': 695,\n",
       " 'feature_389': 696,\n",
       " 'feature_638': 697,\n",
       " 'feature_445': 698,\n",
       " 'feature_342': 699,\n",
       " 'feature_361': 700,\n",
       " 'feature_692': 701,\n",
       " 'feature_481': 702,\n",
       " 'feature_314': 703,\n",
       " 'feature_529': 704,\n",
       " 'feature_121': 705,\n",
       " 'feature_473': 706,\n",
       " 'feature_680': 707,\n",
       " 'feature_202': 708,\n",
       " 'feature_501': 709,\n",
       " 'feature_305': 710,\n",
       " 'feature_612': 711,\n",
       " 'feature_557': 712,\n",
       " 'feature_333': 713,\n",
       " 'feature_257': 714,\n",
       " 'feature_665': 715,\n",
       " 'feature_134': 716,\n",
       " 'feature_639': 717,\n",
       " 'feature_222': 718,\n",
       " 'feature_650': 719,\n",
       " 'feature_285': 720,\n",
       " 'feature_229': 721,\n",
       " 'feature_453': 722,\n",
       " 'feature_585': 723,\n",
       " 'feature_147': 724,\n",
       " 'feature_708': 725,\n",
       " 'feature_693': 726,\n",
       " 'feature_425': 727,\n",
       " 'feature_313': 728,\n",
       " 'feature_174': 729,\n",
       " 'feature_194': 730,\n",
       " 'feature_135': 731,\n",
       " 'feature_666': 732,\n",
       " 'feature_278': 733,\n",
       " 'feature_564': 734,\n",
       " 'feature_120': 735,\n",
       " 'feature_679': 736,\n",
       " 'feature_640': 737,\n",
       " 'feature_397': 738,\n",
       " 'feature_536': 739,\n",
       " 'feature_201': 740,\n",
       " 'feature_613': 741,\n",
       " 'feature_341': 742,\n",
       " 'feature_369': 743,\n",
       " 'feature_256': 744,\n",
       " 'feature_502': 745,\n",
       " 'feature_530': 746,\n",
       " 'feature_508': 747,\n",
       " 'feature_667': 748,\n",
       " 'feature_694': 749,\n",
       " 'feature_558': 750,\n",
       " 'feature_474': 751,\n",
       " 'feature_146': 752,\n",
       " 'feature_312': 753,\n",
       " 'feature_707': 754,\n",
       " 'feature_306': 755,\n",
       " 'feature_678': 756,\n",
       " 'feature_119': 757,\n",
       " 'feature_641': 758,\n",
       " 'feature_586': 759,\n",
       " 'feature_173': 760,\n",
       " 'feature_446': 761,\n",
       " 'feature_480': 762,\n",
       " 'feature_668': 763,\n",
       " 'feature_340': 764,\n",
       " 'feature_695': 765,\n",
       " 'feature_418': 766,\n",
       " 'feature_200': 767,\n",
       " 'feature_614': 768,\n",
       " 'feature_334': 769,\n",
       " 'feature_368': 770,\n",
       " 'feature_145': 771,\n",
       " 'feature_390': 772,\n",
       " 'feature_452': 773,\n",
       " 'feature_118': 774,\n",
       " 'feature_563': 775,\n",
       " 'feature_396': 776,\n",
       " 'feature_696': 777,\n",
       " 'feature_424': 778,\n",
       " 'feature_362': 779,\n",
       " 'feature_669': 780,\n",
       " 'feature_642': 781,\n",
       " 'feature_335': 782,\n",
       " 'feature_391': 783,\n",
       " 'feature_670': 784}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_labels = mnist_df.iloc[:, 0].values\n",
    "fisher_discriminant(mnist_df, mnist_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist_df.iloc[:,1:785]\n",
    "y = mnist_df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b/c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = np.zeros(X.shape[1])\n",
    "for train_idx, test_idx in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    importance += clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = importance / kf.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = {}\n",
    "for idx, x in enumerate(np.argsort(importance)[::-1]):\n",
    "    ranking[f\"feature_{x+1}\"] = idx+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_490': 1,\n",
       " 'feature_436': 2,\n",
       " 'feature_351': 3,\n",
       " 'feature_569': 4,\n",
       " 'feature_431': 5,\n",
       " 'feature_212': 6,\n",
       " 'feature_406': 7,\n",
       " 'feature_99': 8,\n",
       " 'feature_291': 9,\n",
       " 'feature_347': 10,\n",
       " 'feature_157': 11,\n",
       " 'feature_435': 12,\n",
       " 'feature_235': 13,\n",
       " 'feature_656': 14,\n",
       " 'feature_487': 15,\n",
       " 'feature_410': 16,\n",
       " 'feature_156': 17,\n",
       " 'feature_376': 18,\n",
       " 'feature_348': 19,\n",
       " 'feature_240': 20,\n",
       " 'feature_657': 21,\n",
       " 'feature_433': 22,\n",
       " 'feature_485': 23,\n",
       " 'feature_486': 24,\n",
       " 'feature_320': 25,\n",
       " 'feature_489': 26,\n",
       " 'feature_298': 27,\n",
       " 'feature_236': 28,\n",
       " 'feature_522': 29,\n",
       " 'feature_349': 30,\n",
       " 'feature_381': 31,\n",
       " 'feature_517': 32,\n",
       " 'feature_659': 33,\n",
       " 'feature_516': 34,\n",
       " 'feature_551': 35,\n",
       " 'feature_378': 36,\n",
       " 'feature_97': 37,\n",
       " 'feature_345': 38,\n",
       " 'feature_599': 39,\n",
       " 'feature_456': 40,\n",
       " 'feature_570': 41,\n",
       " 'feature_403': 42,\n",
       " 'feature_301': 43,\n",
       " 'feature_491': 44,\n",
       " 'feature_523': 45,\n",
       " 'feature_297': 46,\n",
       " 'feature_658': 47,\n",
       " 'feature_352': 48,\n",
       " 'feature_271': 49,\n",
       " 'feature_430': 50,\n",
       " 'feature_346': 51,\n",
       " 'feature_377': 52,\n",
       " 'feature_150': 53,\n",
       " 'feature_207': 54,\n",
       " 'feature_321': 55,\n",
       " 'feature_515': 56,\n",
       " 'feature_519': 57,\n",
       " 'feature_627': 58,\n",
       " 'feature_325': 59,\n",
       " 'feature_373': 60,\n",
       " 'feature_556': 61,\n",
       " 'feature_103': 62,\n",
       " 'feature_154': 63,\n",
       " 'feature_104': 64,\n",
       " 'feature_313': 65,\n",
       " 'feature_360': 66,\n",
       " 'feature_711': 67,\n",
       " 'feature_187': 68,\n",
       " 'feature_259': 69,\n",
       " 'feature_260': 70,\n",
       " 'feature_261': 71,\n",
       " 'feature_262': 72,\n",
       " 'feature_263': 73,\n",
       " 'feature_258': 74,\n",
       " 'feature_216': 75,\n",
       " 'feature_217': 76,\n",
       " 'feature_257': 77,\n",
       " 'feature_264': 78,\n",
       " 'feature_232': 79,\n",
       " 'feature_233': 80,\n",
       " 'feature_273': 81,\n",
       " 'feature_272': 82,\n",
       " 'feature_234': 83,\n",
       " 'feature_209': 84,\n",
       " 'feature_210': 85,\n",
       " 'feature_211': 86,\n",
       " 'feature_266': 87,\n",
       " 'feature_213': 88,\n",
       " 'feature_214': 89,\n",
       " 'feature_215': 90,\n",
       " 'feature_269': 91,\n",
       " 'feature_268': 92,\n",
       " 'feature_267': 93,\n",
       " 'feature_218': 94,\n",
       " 'feature_265': 95,\n",
       " 'feature_270': 96,\n",
       " 'feature_228': 97,\n",
       " 'feature_227': 98,\n",
       " 'feature_241': 99,\n",
       " 'feature_226': 100,\n",
       " 'feature_225': 101,\n",
       " 'feature_242': 102,\n",
       " 'feature_243': 103,\n",
       " 'feature_244': 104,\n",
       " 'feature_229': 105,\n",
       " 'feature_239': 106,\n",
       " 'feature_245': 107,\n",
       " 'feature_246': 108,\n",
       " 'feature_247': 109,\n",
       " 'feature_248': 110,\n",
       " 'feature_249': 111,\n",
       " 'feature_219': 112,\n",
       " 'feature_251': 113,\n",
       " 'feature_224': 114,\n",
       " 'feature_238': 115,\n",
       " 'feature_223': 116,\n",
       " 'feature_222': 117,\n",
       " 'feature_252': 118,\n",
       " 'feature_253': 119,\n",
       " 'feature_237': 120,\n",
       " 'feature_230': 121,\n",
       " 'feature_221': 122,\n",
       " 'feature_254': 123,\n",
       " 'feature_220': 124,\n",
       " 'feature_231': 125,\n",
       " 'feature_255': 126,\n",
       " 'feature_256': 127,\n",
       " 'feature_250': 128,\n",
       " 'feature_344': 129,\n",
       " 'feature_343': 130,\n",
       " 'feature_342': 131,\n",
       " 'feature_341': 132,\n",
       " 'feature_340': 133,\n",
       " 'feature_339': 134,\n",
       " 'feature_338': 135,\n",
       " 'feature_337': 136,\n",
       " 'feature_336': 137,\n",
       " 'feature_208': 138,\n",
       " 'feature_334': 139,\n",
       " 'feature_333': 140,\n",
       " 'feature_332': 141,\n",
       " 'feature_331': 142,\n",
       " 'feature_330': 143,\n",
       " 'feature_329': 144,\n",
       " 'feature_328': 145,\n",
       " 'feature_327': 146,\n",
       " 'feature_326': 147,\n",
       " 'feature_324': 148,\n",
       " 'feature_335': 149,\n",
       " 'feature_374': 150,\n",
       " 'feature_372': 151,\n",
       " 'feature_371': 152,\n",
       " 'feature_370': 153,\n",
       " 'feature_369': 154,\n",
       " 'feature_368': 155,\n",
       " 'feature_367': 156,\n",
       " 'feature_366': 157,\n",
       " 'feature_365': 158,\n",
       " 'feature_350': 159,\n",
       " 'feature_363': 160,\n",
       " 'feature_362': 161,\n",
       " 'feature_361': 162,\n",
       " 'feature_359': 163,\n",
       " 'feature_358': 164,\n",
       " 'feature_357': 165,\n",
       " 'feature_356': 166,\n",
       " 'feature_355': 167,\n",
       " 'feature_354': 168,\n",
       " 'feature_353': 169,\n",
       " 'feature_364': 170,\n",
       " 'feature_294': 171,\n",
       " 'feature_293': 172,\n",
       " 'feature_292': 173,\n",
       " 'feature_290': 174,\n",
       " 'feature_289': 175,\n",
       " 'feature_288': 176,\n",
       " 'feature_287': 177,\n",
       " 'feature_286': 178,\n",
       " 'feature_285': 179,\n",
       " 'feature_323': 180,\n",
       " 'feature_283': 181,\n",
       " 'feature_282': 182,\n",
       " 'feature_281': 183,\n",
       " 'feature_280': 184,\n",
       " 'feature_279': 185,\n",
       " 'feature_278': 186,\n",
       " 'feature_277': 187,\n",
       " 'feature_276': 188,\n",
       " 'feature_275': 189,\n",
       " 'feature_274': 190,\n",
       " 'feature_284': 191,\n",
       " 'feature_322': 192,\n",
       " 'feature_319': 193,\n",
       " 'feature_318': 194,\n",
       " 'feature_317': 195,\n",
       " 'feature_316': 196,\n",
       " 'feature_315': 197,\n",
       " 'feature_314': 198,\n",
       " 'feature_312': 199,\n",
       " 'feature_311': 200,\n",
       " 'feature_310': 201,\n",
       " 'feature_295': 202,\n",
       " 'feature_308': 203,\n",
       " 'feature_307': 204,\n",
       " 'feature_306': 205,\n",
       " 'feature_305': 206,\n",
       " 'feature_304': 207,\n",
       " 'feature_303': 208,\n",
       " 'feature_302': 209,\n",
       " 'feature_300': 210,\n",
       " 'feature_299': 211,\n",
       " 'feature_296': 212,\n",
       " 'feature_309': 213,\n",
       " 'feature_73': 214,\n",
       " 'feature_72': 215,\n",
       " 'feature_71': 216,\n",
       " 'feature_70': 217,\n",
       " 'feature_69': 218,\n",
       " 'feature_68': 219,\n",
       " 'feature_67': 220,\n",
       " 'feature_66': 221,\n",
       " 'feature_65': 222,\n",
       " 'feature_64': 223,\n",
       " 'feature_63': 224,\n",
       " 'feature_199': 225,\n",
       " 'feature_61': 226,\n",
       " 'feature_60': 227,\n",
       " 'feature_59': 228,\n",
       " 'feature_58': 229,\n",
       " 'feature_57': 230,\n",
       " 'feature_56': 231,\n",
       " 'feature_55': 232,\n",
       " 'feature_54': 233,\n",
       " 'feature_53': 234,\n",
       " 'feature_52': 235,\n",
       " 'feature_51': 236,\n",
       " 'feature_62': 237,\n",
       " 'feature_100': 238,\n",
       " 'feature_98': 239,\n",
       " 'feature_96': 240,\n",
       " 'feature_95': 241,\n",
       " 'feature_94': 242,\n",
       " 'feature_93': 243,\n",
       " 'feature_92': 244,\n",
       " 'feature_91': 245,\n",
       " 'feature_90': 246,\n",
       " 'feature_89': 247,\n",
       " 'feature_88': 248,\n",
       " 'feature_74': 249,\n",
       " 'feature_86': 250,\n",
       " 'feature_85': 251,\n",
       " 'feature_84': 252,\n",
       " 'feature_83': 253,\n",
       " 'feature_82': 254,\n",
       " 'feature_81': 255,\n",
       " 'feature_80': 256,\n",
       " 'feature_79': 257,\n",
       " 'feature_78': 258,\n",
       " 'feature_77': 259,\n",
       " 'feature_76': 260,\n",
       " 'feature_75': 261,\n",
       " 'feature_87': 262,\n",
       " 'feature_24': 263,\n",
       " 'feature_23': 264,\n",
       " 'feature_22': 265,\n",
       " 'feature_21': 266,\n",
       " 'feature_20': 267,\n",
       " 'feature_19': 268,\n",
       " 'feature_18': 269,\n",
       " 'feature_17': 270,\n",
       " 'feature_16': 271,\n",
       " 'feature_15': 272,\n",
       " 'feature_14': 273,\n",
       " 'feature_50': 274,\n",
       " 'feature_12': 275,\n",
       " 'feature_11': 276,\n",
       " 'feature_10': 277,\n",
       " 'feature_9': 278,\n",
       " 'feature_8': 279,\n",
       " 'feature_7': 280,\n",
       " 'feature_6': 281,\n",
       " 'feature_5': 282,\n",
       " 'feature_4': 283,\n",
       " 'feature_3': 284,\n",
       " 'feature_2': 285,\n",
       " 'feature_13': 286,\n",
       " 'feature_49': 287,\n",
       " 'feature_48': 288,\n",
       " 'feature_47': 289,\n",
       " 'feature_46': 290,\n",
       " 'feature_45': 291,\n",
       " 'feature_44': 292,\n",
       " 'feature_43': 293,\n",
       " 'feature_42': 294,\n",
       " 'feature_41': 295,\n",
       " 'feature_40': 296,\n",
       " 'feature_39': 297,\n",
       " 'feature_25': 298,\n",
       " 'feature_37': 299,\n",
       " 'feature_36': 300,\n",
       " 'feature_35': 301,\n",
       " 'feature_34': 302,\n",
       " 'feature_33': 303,\n",
       " 'feature_32': 304,\n",
       " 'feature_31': 305,\n",
       " 'feature_30': 306,\n",
       " 'feature_29': 307,\n",
       " 'feature_28': 308,\n",
       " 'feature_27': 309,\n",
       " 'feature_26': 310,\n",
       " 'feature_38': 311,\n",
       " 'feature_180': 312,\n",
       " 'feature_179': 313,\n",
       " 'feature_178': 314,\n",
       " 'feature_177': 315,\n",
       " 'feature_176': 316,\n",
       " 'feature_175': 317,\n",
       " 'feature_174': 318,\n",
       " 'feature_173': 319,\n",
       " 'feature_172': 320,\n",
       " 'feature_171': 321,\n",
       " 'feature_170': 322,\n",
       " 'feature_101': 323,\n",
       " 'feature_168': 324,\n",
       " 'feature_167': 325,\n",
       " 'feature_166': 326,\n",
       " 'feature_165': 327,\n",
       " 'feature_164': 328,\n",
       " 'feature_163': 329,\n",
       " 'feature_162': 330,\n",
       " 'feature_161': 331,\n",
       " 'feature_160': 332,\n",
       " 'feature_159': 333,\n",
       " 'feature_158': 334,\n",
       " 'feature_169': 335,\n",
       " 'feature_206': 336,\n",
       " 'feature_205': 337,\n",
       " 'feature_204': 338,\n",
       " 'feature_203': 339,\n",
       " 'feature_202': 340,\n",
       " 'feature_201': 341,\n",
       " 'feature_200': 342,\n",
       " 'feature_375': 343,\n",
       " 'feature_198': 344,\n",
       " 'feature_197': 345,\n",
       " 'feature_196': 346,\n",
       " 'feature_181': 347,\n",
       " 'feature_194': 348,\n",
       " 'feature_193': 349,\n",
       " 'feature_192': 350,\n",
       " 'feature_191': 351,\n",
       " 'feature_190': 352,\n",
       " 'feature_189': 353,\n",
       " 'feature_188': 354,\n",
       " 'feature_186': 355,\n",
       " 'feature_185': 356,\n",
       " 'feature_184': 357,\n",
       " 'feature_183': 358,\n",
       " 'feature_182': 359,\n",
       " 'feature_195': 360,\n",
       " 'feature_127': 361,\n",
       " 'feature_126': 362,\n",
       " 'feature_125': 363,\n",
       " 'feature_124': 364,\n",
       " 'feature_123': 365,\n",
       " 'feature_122': 366,\n",
       " 'feature_121': 367,\n",
       " 'feature_120': 368,\n",
       " 'feature_119': 369,\n",
       " 'feature_118': 370,\n",
       " 'feature_117': 371,\n",
       " 'feature_155': 372,\n",
       " 'feature_115': 373,\n",
       " 'feature_114': 374,\n",
       " 'feature_113': 375,\n",
       " 'feature_112': 376,\n",
       " 'feature_111': 377,\n",
       " 'feature_110': 378,\n",
       " 'feature_109': 379,\n",
       " 'feature_108': 380,\n",
       " 'feature_107': 381,\n",
       " 'feature_106': 382,\n",
       " 'feature_105': 383,\n",
       " 'feature_102': 384,\n",
       " 'feature_116': 385,\n",
       " 'feature_153': 386,\n",
       " 'feature_152': 387,\n",
       " 'feature_151': 388,\n",
       " 'feature_149': 389,\n",
       " 'feature_148': 390,\n",
       " 'feature_147': 391,\n",
       " 'feature_146': 392,\n",
       " 'feature_145': 393,\n",
       " 'feature_144': 394,\n",
       " 'feature_143': 395,\n",
       " 'feature_142': 396,\n",
       " 'feature_128': 397,\n",
       " 'feature_140': 398,\n",
       " 'feature_139': 399,\n",
       " 'feature_138': 400,\n",
       " 'feature_137': 401,\n",
       " 'feature_136': 402,\n",
       " 'feature_135': 403,\n",
       " 'feature_134': 404,\n",
       " 'feature_133': 405,\n",
       " 'feature_132': 406,\n",
       " 'feature_131': 407,\n",
       " 'feature_130': 408,\n",
       " 'feature_129': 409,\n",
       " 'feature_141': 410,\n",
       " 'feature_666': 411,\n",
       " 'feature_665': 412,\n",
       " 'feature_664': 413,\n",
       " 'feature_663': 414,\n",
       " 'feature_662': 415,\n",
       " 'feature_661': 416,\n",
       " 'feature_660': 417,\n",
       " 'feature_655': 418,\n",
       " 'feature_654': 419,\n",
       " 'feature_653': 420,\n",
       " 'feature_393': 421,\n",
       " 'feature_651': 422,\n",
       " 'feature_650': 423,\n",
       " 'feature_649': 424,\n",
       " 'feature_648': 425,\n",
       " 'feature_647': 426,\n",
       " 'feature_646': 427,\n",
       " 'feature_645': 428,\n",
       " 'feature_644': 429,\n",
       " 'feature_643': 430,\n",
       " 'feature_642': 431,\n",
       " 'feature_641': 432,\n",
       " 'feature_652': 433,\n",
       " 'feature_689': 434,\n",
       " 'feature_688': 435,\n",
       " 'feature_687': 436,\n",
       " 'feature_686': 437,\n",
       " 'feature_685': 438,\n",
       " 'feature_684': 439,\n",
       " 'feature_683': 440,\n",
       " 'feature_682': 441,\n",
       " 'feature_681': 442,\n",
       " 'feature_680': 443,\n",
       " 'feature_667': 444,\n",
       " 'feature_678': 445,\n",
       " 'feature_677': 446,\n",
       " 'feature_676': 447,\n",
       " 'feature_675': 448,\n",
       " 'feature_674': 449,\n",
       " 'feature_673': 450,\n",
       " 'feature_672': 451,\n",
       " 'feature_671': 452,\n",
       " 'feature_670': 453,\n",
       " 'feature_669': 454,\n",
       " 'feature_668': 455,\n",
       " 'feature_679': 456,\n",
       " 'feature_614': 457,\n",
       " 'feature_613': 458,\n",
       " 'feature_612': 459,\n",
       " 'feature_611': 460,\n",
       " 'feature_610': 461,\n",
       " 'feature_609': 462,\n",
       " 'feature_608': 463,\n",
       " 'feature_607': 464,\n",
       " 'feature_606': 465,\n",
       " 'feature_605': 466,\n",
       " 'feature_640': 467,\n",
       " 'feature_603': 468,\n",
       " 'feature_602': 469,\n",
       " 'feature_601': 470,\n",
       " 'feature_600': 471,\n",
       " 'feature_598': 472,\n",
       " 'feature_597': 473,\n",
       " 'feature_596': 474,\n",
       " 'feature_595': 475,\n",
       " 'feature_594': 476,\n",
       " 'feature_593': 477,\n",
       " 'feature_592': 478,\n",
       " 'feature_604': 479,\n",
       " 'feature_639': 480,\n",
       " 'feature_638': 481,\n",
       " 'feature_637': 482,\n",
       " 'feature_636': 483,\n",
       " 'feature_635': 484,\n",
       " 'feature_634': 485,\n",
       " 'feature_633': 486,\n",
       " 'feature_632': 487,\n",
       " 'feature_631': 488,\n",
       " 'feature_630': 489,\n",
       " 'feature_629': 490,\n",
       " 'feature_615': 491,\n",
       " 'feature_626': 492,\n",
       " 'feature_625': 493,\n",
       " 'feature_624': 494,\n",
       " 'feature_623': 495,\n",
       " 'feature_622': 496,\n",
       " 'feature_621': 497,\n",
       " 'feature_620': 498,\n",
       " 'feature_619': 499,\n",
       " 'feature_618': 500,\n",
       " 'feature_617': 501,\n",
       " 'feature_616': 502,\n",
       " 'feature_628': 503,\n",
       " 'feature_760': 504,\n",
       " 'feature_759': 505,\n",
       " 'feature_758': 506,\n",
       " 'feature_757': 507,\n",
       " 'feature_756': 508,\n",
       " 'feature_755': 509,\n",
       " 'feature_754': 510,\n",
       " 'feature_753': 511,\n",
       " 'feature_752': 512,\n",
       " 'feature_751': 513,\n",
       " 'feature_690': 514,\n",
       " 'feature_749': 515,\n",
       " 'feature_748': 516,\n",
       " 'feature_747': 517,\n",
       " 'feature_746': 518,\n",
       " 'feature_745': 519,\n",
       " 'feature_744': 520,\n",
       " 'feature_743': 521,\n",
       " 'feature_742': 522,\n",
       " 'feature_741': 523,\n",
       " 'feature_740': 524,\n",
       " 'feature_739': 525,\n",
       " 'feature_750': 526,\n",
       " 'feature_784': 527,\n",
       " 'feature_783': 528,\n",
       " 'feature_782': 529,\n",
       " 'feature_781': 530,\n",
       " 'feature_780': 531,\n",
       " 'feature_779': 532,\n",
       " 'feature_778': 533,\n",
       " 'feature_777': 534,\n",
       " 'feature_776': 535,\n",
       " 'feature_775': 536,\n",
       " 'feature_774': 537,\n",
       " 'feature_761': 538,\n",
       " 'feature_772': 539,\n",
       " 'feature_771': 540,\n",
       " 'feature_770': 541,\n",
       " 'feature_769': 542,\n",
       " 'feature_768': 543,\n",
       " 'feature_767': 544,\n",
       " 'feature_766': 545,\n",
       " 'feature_765': 546,\n",
       " 'feature_764': 547,\n",
       " 'feature_763': 548,\n",
       " 'feature_762': 549,\n",
       " 'feature_773': 550,\n",
       " 'feature_713': 551,\n",
       " 'feature_712': 552,\n",
       " 'feature_710': 553,\n",
       " 'feature_709': 554,\n",
       " 'feature_708': 555,\n",
       " 'feature_707': 556,\n",
       " 'feature_706': 557,\n",
       " 'feature_705': 558,\n",
       " 'feature_704': 559,\n",
       " 'feature_703': 560,\n",
       " 'feature_738': 561,\n",
       " 'feature_701': 562,\n",
       " 'feature_700': 563,\n",
       " 'feature_699': 564,\n",
       " 'feature_698': 565,\n",
       " 'feature_697': 566,\n",
       " 'feature_696': 567,\n",
       " 'feature_695': 568,\n",
       " 'feature_694': 569,\n",
       " 'feature_693': 570,\n",
       " 'feature_692': 571,\n",
       " 'feature_691': 572,\n",
       " 'feature_702': 573,\n",
       " 'feature_737': 574,\n",
       " 'feature_736': 575,\n",
       " 'feature_735': 576,\n",
       " 'feature_734': 577,\n",
       " 'feature_733': 578,\n",
       " 'feature_732': 579,\n",
       " 'feature_731': 580,\n",
       " 'feature_730': 581,\n",
       " 'feature_729': 582,\n",
       " 'feature_728': 583,\n",
       " 'feature_727': 584,\n",
       " 'feature_714': 585,\n",
       " 'feature_725': 586,\n",
       " 'feature_724': 587,\n",
       " 'feature_723': 588,\n",
       " 'feature_722': 589,\n",
       " 'feature_721': 590,\n",
       " 'feature_720': 591,\n",
       " 'feature_719': 592,\n",
       " 'feature_718': 593,\n",
       " 'feature_717': 594,\n",
       " 'feature_716': 595,\n",
       " 'feature_715': 596,\n",
       " 'feature_726': 597,\n",
       " 'feature_457': 598,\n",
       " 'feature_455': 599,\n",
       " 'feature_454': 600,\n",
       " 'feature_453': 601,\n",
       " 'feature_452': 602,\n",
       " 'feature_451': 603,\n",
       " 'feature_450': 604,\n",
       " 'feature_449': 605,\n",
       " 'feature_448': 606,\n",
       " 'feature_447': 607,\n",
       " 'feature_591': 608,\n",
       " 'feature_445': 609,\n",
       " 'feature_444': 610,\n",
       " 'feature_443': 611,\n",
       " 'feature_442': 612,\n",
       " 'feature_441': 613,\n",
       " 'feature_440': 614,\n",
       " 'feature_439': 615,\n",
       " 'feature_438': 616,\n",
       " 'feature_437': 617,\n",
       " 'feature_434': 618,\n",
       " 'feature_432': 619,\n",
       " 'feature_446': 620,\n",
       " 'feature_480': 621,\n",
       " 'feature_479': 622,\n",
       " 'feature_478': 623,\n",
       " 'feature_477': 624,\n",
       " 'feature_476': 625,\n",
       " 'feature_475': 626,\n",
       " 'feature_474': 627,\n",
       " 'feature_473': 628,\n",
       " 'feature_472': 629,\n",
       " 'feature_471': 630,\n",
       " 'feature_458': 631,\n",
       " 'feature_469': 632,\n",
       " 'feature_468': 633,\n",
       " 'feature_467': 634,\n",
       " 'feature_466': 635,\n",
       " 'feature_465': 636,\n",
       " 'feature_464': 637,\n",
       " 'feature_463': 638,\n",
       " 'feature_462': 639,\n",
       " 'feature_461': 640,\n",
       " 'feature_460': 641,\n",
       " 'feature_459': 642,\n",
       " 'feature_470': 643,\n",
       " 'feature_401': 644,\n",
       " 'feature_400': 645,\n",
       " 'feature_399': 646,\n",
       " 'feature_398': 647,\n",
       " 'feature_397': 648,\n",
       " 'feature_396': 649,\n",
       " 'feature_395': 650,\n",
       " 'feature_394': 651,\n",
       " 'feature_1': 652,\n",
       " 'feature_392': 653,\n",
       " 'feature_429': 654,\n",
       " 'feature_390': 655,\n",
       " 'feature_389': 656,\n",
       " 'feature_388': 657,\n",
       " 'feature_387': 658,\n",
       " 'feature_386': 659,\n",
       " 'feature_385': 660,\n",
       " 'feature_384': 661,\n",
       " 'feature_383': 662,\n",
       " 'feature_382': 663,\n",
       " 'feature_380': 664,\n",
       " 'feature_379': 665,\n",
       " 'feature_391': 666,\n",
       " 'feature_428': 667,\n",
       " 'feature_427': 668,\n",
       " 'feature_426': 669,\n",
       " 'feature_425': 670,\n",
       " 'feature_424': 671,\n",
       " 'feature_423': 672,\n",
       " 'feature_422': 673,\n",
       " 'feature_421': 674,\n",
       " 'feature_420': 675,\n",
       " 'feature_419': 676,\n",
       " 'feature_418': 677,\n",
       " 'feature_402': 678,\n",
       " 'feature_416': 679,\n",
       " 'feature_415': 680,\n",
       " 'feature_414': 681,\n",
       " 'feature_413': 682,\n",
       " 'feature_412': 683,\n",
       " 'feature_411': 684,\n",
       " 'feature_409': 685,\n",
       " 'feature_408': 686,\n",
       " 'feature_407': 687,\n",
       " 'feature_405': 688,\n",
       " 'feature_404': 689,\n",
       " 'feature_417': 690,\n",
       " 'feature_564': 691,\n",
       " 'feature_563': 692,\n",
       " 'feature_562': 693,\n",
       " 'feature_561': 694,\n",
       " 'feature_560': 695,\n",
       " 'feature_559': 696,\n",
       " 'feature_558': 697,\n",
       " 'feature_557': 698,\n",
       " 'feature_555': 699,\n",
       " 'feature_554': 700,\n",
       " 'feature_481': 701,\n",
       " 'feature_552': 702,\n",
       " 'feature_550': 703,\n",
       " 'feature_549': 704,\n",
       " 'feature_548': 705,\n",
       " 'feature_547': 706,\n",
       " 'feature_546': 707,\n",
       " 'feature_545': 708,\n",
       " 'feature_544': 709,\n",
       " 'feature_543': 710,\n",
       " 'feature_542': 711,\n",
       " 'feature_541': 712,\n",
       " 'feature_553': 713,\n",
       " 'feature_590': 714,\n",
       " 'feature_589': 715,\n",
       " 'feature_588': 716,\n",
       " 'feature_587': 717,\n",
       " 'feature_586': 718,\n",
       " 'feature_585': 719,\n",
       " 'feature_584': 720,\n",
       " 'feature_583': 721,\n",
       " 'feature_582': 722,\n",
       " 'feature_581': 723,\n",
       " 'feature_580': 724,\n",
       " 'feature_565': 725,\n",
       " 'feature_578': 726,\n",
       " 'feature_577': 727,\n",
       " 'feature_576': 728,\n",
       " 'feature_575': 729,\n",
       " 'feature_574': 730,\n",
       " 'feature_573': 731,\n",
       " 'feature_572': 732,\n",
       " 'feature_571': 733,\n",
       " 'feature_568': 734,\n",
       " 'feature_567': 735,\n",
       " 'feature_566': 736,\n",
       " 'feature_579': 737,\n",
       " 'feature_509': 738,\n",
       " 'feature_508': 739,\n",
       " 'feature_507': 740,\n",
       " 'feature_506': 741,\n",
       " 'feature_505': 742,\n",
       " 'feature_504': 743,\n",
       " 'feature_503': 744,\n",
       " 'feature_502': 745,\n",
       " 'feature_501': 746,\n",
       " 'feature_500': 747,\n",
       " 'feature_540': 748,\n",
       " 'feature_498': 749,\n",
       " 'feature_497': 750,\n",
       " 'feature_496': 751,\n",
       " 'feature_495': 752,\n",
       " 'feature_494': 753,\n",
       " 'feature_493': 754,\n",
       " 'feature_492': 755,\n",
       " 'feature_488': 756,\n",
       " 'feature_484': 757,\n",
       " 'feature_483': 758,\n",
       " 'feature_482': 759,\n",
       " 'feature_499': 760,\n",
       " 'feature_539': 761,\n",
       " 'feature_538': 762,\n",
       " 'feature_537': 763,\n",
       " 'feature_536': 764,\n",
       " 'feature_535': 765,\n",
       " 'feature_534': 766,\n",
       " 'feature_533': 767,\n",
       " 'feature_532': 768,\n",
       " 'feature_531': 769,\n",
       " 'feature_530': 770,\n",
       " 'feature_529': 771,\n",
       " 'feature_510': 772,\n",
       " 'feature_527': 773,\n",
       " 'feature_526': 774,\n",
       " 'feature_525': 775,\n",
       " 'feature_524': 776,\n",
       " 'feature_521': 777,\n",
       " 'feature_520': 778,\n",
       " 'feature_518': 779,\n",
       " 'feature_514': 780,\n",
       " 'feature_513': 781,\n",
       " 'feature_512': 782,\n",
       " 'feature_511': 783,\n",
       " 'feature_528': 784}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decision_tree.pdf'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data = export_graphviz(clf, out_file=None, \n",
    "                           filled=True, rounded=True, \n",
    "                           special_characters=True, \n",
    "                           feature_names=[f'pixel_{int(i)}' for i in range(X.shape[1])], \n",
    "                           class_names=[str(i) for i in clf.classes_])\n",
    "\n",
    "graph = graphviz.Source(graph_data)\n",
    "graph.render(\"decision_tree\")\n",
    "graph.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: provide analysis on the decision tree and the runtime complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f.\n",
    "TODO: Compare the feature importances from the two methods. Consider what sets the two methods apart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features42k_df = pd.read_excel(\"trainFeatures42k.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41999, 61)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features42k_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_feature42k= features42k_df.iloc[:, 0]\n",
    "X_features42 = features42k_df.iloc[:, 1:61]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_min_max(df, a=0, b=1):\n",
    "    # df: df.DataFrame -> dataset\n",
    "\n",
    "    min_xk = np.min(df.values, axis=0)\n",
    "    max_xk = np.max(df.values, axis=0)\n",
    "\n",
    "    # Min-max normalization\n",
    "    xhat = (df.values - min_xk) / (max_xk - min_xk) * (b-a) + a\n",
    "\n",
    "    normalized_df = pd.DataFrame(data=xhat, columns=df.columns)\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_features42k = normalize_min_max(features42k_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.000000</th>\n",
       "      <th>538.267964</th>\n",
       "      <th>-314.023125</th>\n",
       "      <th>443.809967</th>\n",
       "      <th>470.780028</th>\n",
       "      <th>176.561668</th>\n",
       "      <th>-336.130920</th>\n",
       "      <th>23.221391</th>\n",
       "      <th>-45.523748</th>\n",
       "      <th>-232.436917</th>\n",
       "      <th>...</th>\n",
       "      <th>-90.463868</th>\n",
       "      <th>107.934027</th>\n",
       "      <th>25.417533</th>\n",
       "      <th>-97.235438</th>\n",
       "      <th>-66.589588</th>\n",
       "      <th>22.468479</th>\n",
       "      <th>-111.476083</th>\n",
       "      <th>62.807185</th>\n",
       "      <th>74.771969</th>\n",
       "      <th>-7.480156</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283821</td>\n",
       "      <td>0.749112</td>\n",
       "      <td>0.481985</td>\n",
       "      <td>0.528984</td>\n",
       "      <td>0.261973</td>\n",
       "      <td>0.468056</td>\n",
       "      <td>0.265366</td>\n",
       "      <td>0.558557</td>\n",
       "      <td>0.486359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537583</td>\n",
       "      <td>0.527849</td>\n",
       "      <td>0.478293</td>\n",
       "      <td>0.587293</td>\n",
       "      <td>0.509707</td>\n",
       "      <td>0.514439</td>\n",
       "      <td>0.544272</td>\n",
       "      <td>0.511564</td>\n",
       "      <td>0.320550</td>\n",
       "      <td>0.540833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.409536</td>\n",
       "      <td>0.624123</td>\n",
       "      <td>0.488822</td>\n",
       "      <td>0.491783</td>\n",
       "      <td>0.502634</td>\n",
       "      <td>0.478842</td>\n",
       "      <td>0.430549</td>\n",
       "      <td>0.539927</td>\n",
       "      <td>0.546484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497741</td>\n",
       "      <td>0.530866</td>\n",
       "      <td>0.463589</td>\n",
       "      <td>0.557306</td>\n",
       "      <td>0.514205</td>\n",
       "      <td>0.481619</td>\n",
       "      <td>0.585617</td>\n",
       "      <td>0.534069</td>\n",
       "      <td>0.407364</td>\n",
       "      <td>0.497220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.435185</td>\n",
       "      <td>0.668845</td>\n",
       "      <td>0.537963</td>\n",
       "      <td>0.506334</td>\n",
       "      <td>0.547429</td>\n",
       "      <td>0.592446</td>\n",
       "      <td>0.412361</td>\n",
       "      <td>0.428013</td>\n",
       "      <td>0.590881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522707</td>\n",
       "      <td>0.493350</td>\n",
       "      <td>0.403367</td>\n",
       "      <td>0.559033</td>\n",
       "      <td>0.477097</td>\n",
       "      <td>0.330888</td>\n",
       "      <td>0.517985</td>\n",
       "      <td>0.472785</td>\n",
       "      <td>0.414247</td>\n",
       "      <td>0.492908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257167</td>\n",
       "      <td>0.713353</td>\n",
       "      <td>0.541088</td>\n",
       "      <td>0.562749</td>\n",
       "      <td>0.132652</td>\n",
       "      <td>0.555744</td>\n",
       "      <td>0.377126</td>\n",
       "      <td>0.457405</td>\n",
       "      <td>0.489312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487022</td>\n",
       "      <td>0.551943</td>\n",
       "      <td>0.449962</td>\n",
       "      <td>0.561684</td>\n",
       "      <td>0.444016</td>\n",
       "      <td>0.433361</td>\n",
       "      <td>0.590519</td>\n",
       "      <td>0.584408</td>\n",
       "      <td>0.413019</td>\n",
       "      <td>0.422263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407658</td>\n",
       "      <td>0.592930</td>\n",
       "      <td>0.459606</td>\n",
       "      <td>0.465729</td>\n",
       "      <td>0.362588</td>\n",
       "      <td>0.323096</td>\n",
       "      <td>0.298780</td>\n",
       "      <td>0.486134</td>\n",
       "      <td>0.469550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407590</td>\n",
       "      <td>0.530187</td>\n",
       "      <td>0.428120</td>\n",
       "      <td>0.502922</td>\n",
       "      <td>0.458857</td>\n",
       "      <td>0.468250</td>\n",
       "      <td>0.671773</td>\n",
       "      <td>0.464809</td>\n",
       "      <td>0.433602</td>\n",
       "      <td>0.542738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41994</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.237773</td>\n",
       "      <td>0.643146</td>\n",
       "      <td>0.494709</td>\n",
       "      <td>0.682049</td>\n",
       "      <td>0.062825</td>\n",
       "      <td>0.576904</td>\n",
       "      <td>0.589713</td>\n",
       "      <td>0.376516</td>\n",
       "      <td>0.464114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.683423</td>\n",
       "      <td>0.531347</td>\n",
       "      <td>0.430077</td>\n",
       "      <td>0.585305</td>\n",
       "      <td>0.469920</td>\n",
       "      <td>0.415228</td>\n",
       "      <td>0.586375</td>\n",
       "      <td>0.539669</td>\n",
       "      <td>0.364866</td>\n",
       "      <td>0.246553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.429802</td>\n",
       "      <td>0.567614</td>\n",
       "      <td>0.557314</td>\n",
       "      <td>0.524615</td>\n",
       "      <td>0.513880</td>\n",
       "      <td>0.451169</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>0.512131</td>\n",
       "      <td>0.557082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510879</td>\n",
       "      <td>0.541927</td>\n",
       "      <td>0.472294</td>\n",
       "      <td>0.560941</td>\n",
       "      <td>0.544500</td>\n",
       "      <td>0.521826</td>\n",
       "      <td>0.610126</td>\n",
       "      <td>0.510385</td>\n",
       "      <td>0.407463</td>\n",
       "      <td>0.431950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.267067</td>\n",
       "      <td>0.645404</td>\n",
       "      <td>0.560368</td>\n",
       "      <td>0.334379</td>\n",
       "      <td>0.480393</td>\n",
       "      <td>0.221296</td>\n",
       "      <td>0.502481</td>\n",
       "      <td>0.402787</td>\n",
       "      <td>0.771513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510933</td>\n",
       "      <td>0.464269</td>\n",
       "      <td>0.253415</td>\n",
       "      <td>0.474281</td>\n",
       "      <td>0.604744</td>\n",
       "      <td>0.487935</td>\n",
       "      <td>0.714094</td>\n",
       "      <td>0.343001</td>\n",
       "      <td>0.379957</td>\n",
       "      <td>0.417166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.272818</td>\n",
       "      <td>0.517168</td>\n",
       "      <td>0.191589</td>\n",
       "      <td>0.490126</td>\n",
       "      <td>0.535226</td>\n",
       "      <td>0.628212</td>\n",
       "      <td>0.484334</td>\n",
       "      <td>0.709866</td>\n",
       "      <td>0.556909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373141</td>\n",
       "      <td>0.316531</td>\n",
       "      <td>0.486255</td>\n",
       "      <td>0.628320</td>\n",
       "      <td>0.524673</td>\n",
       "      <td>0.572347</td>\n",
       "      <td>0.546865</td>\n",
       "      <td>0.463921</td>\n",
       "      <td>0.352464</td>\n",
       "      <td>0.433093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.454206</td>\n",
       "      <td>0.551745</td>\n",
       "      <td>0.454368</td>\n",
       "      <td>0.706532</td>\n",
       "      <td>0.438111</td>\n",
       "      <td>0.504709</td>\n",
       "      <td>0.603627</td>\n",
       "      <td>0.574112</td>\n",
       "      <td>0.696341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689389</td>\n",
       "      <td>0.547013</td>\n",
       "      <td>0.549546</td>\n",
       "      <td>0.613893</td>\n",
       "      <td>0.396463</td>\n",
       "      <td>0.532193</td>\n",
       "      <td>0.659391</td>\n",
       "      <td>0.541830</td>\n",
       "      <td>0.436454</td>\n",
       "      <td>0.546414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41999 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1.000000     538.267964  -314.023125   443.809967   470.780028  \\\n",
       "0         0.000000     0.283821     0.749112     0.481985     0.528984   \n",
       "1         0.111111     0.409536     0.624123     0.488822     0.491783   \n",
       "2         0.444444     0.435185     0.668845     0.537963     0.506334   \n",
       "3         0.000000     0.257167     0.713353     0.541088     0.562749   \n",
       "4         0.000000     0.407658     0.592930     0.459606     0.465729   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "41994     0.000000     0.237773     0.643146     0.494709     0.682049   \n",
       "41995     0.111111     0.429802     0.567614     0.557314     0.524615   \n",
       "41996     0.777778     0.267067     0.645404     0.560368     0.334379   \n",
       "41997     0.666667     0.272818     0.517168     0.191589     0.490126   \n",
       "41998     1.000000     0.454206     0.551745     0.454368     0.706532   \n",
       "\n",
       "        176.561668  -336.130920   23.221391   -45.523748   -232.436917  ...  \\\n",
       "0         0.261973     0.468056     0.265366     0.558557     0.486359  ...   \n",
       "1         0.502634     0.478842     0.430549     0.539927     0.546484  ...   \n",
       "2         0.547429     0.592446     0.412361     0.428013     0.590881  ...   \n",
       "3         0.132652     0.555744     0.377126     0.457405     0.489312  ...   \n",
       "4         0.362588     0.323096     0.298780     0.486134     0.469550  ...   \n",
       "...            ...          ...          ...          ...          ...  ...   \n",
       "41994     0.062825     0.576904     0.589713     0.376516     0.464114  ...   \n",
       "41995     0.513880     0.451169     0.526000     0.512131     0.557082  ...   \n",
       "41996     0.480393     0.221296     0.502481     0.402787     0.771513  ...   \n",
       "41997     0.535226     0.628212     0.484334     0.709866     0.556909  ...   \n",
       "41998     0.438111     0.504709     0.603627     0.574112     0.696341  ...   \n",
       "\n",
       "       -90.463868    107.934027   25.417533   -97.235438   -66.589588   \\\n",
       "0         0.537583     0.527849     0.478293     0.587293     0.509707   \n",
       "1         0.497741     0.530866     0.463589     0.557306     0.514205   \n",
       "2         0.522707     0.493350     0.403367     0.559033     0.477097   \n",
       "3         0.487022     0.551943     0.449962     0.561684     0.444016   \n",
       "4         0.407590     0.530187     0.428120     0.502922     0.458857   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "41994     0.683423     0.531347     0.430077     0.585305     0.469920   \n",
       "41995     0.510879     0.541927     0.472294     0.560941     0.544500   \n",
       "41996     0.510933     0.464269     0.253415     0.474281     0.604744   \n",
       "41997     0.373141     0.316531     0.486255     0.628320     0.524673   \n",
       "41998     0.689389     0.547013     0.549546     0.613893     0.396463   \n",
       "\n",
       "        22.468479   -111.476083   62.807185    74.771969   -7.480156    \n",
       "0         0.514439     0.544272     0.511564     0.320550     0.540833  \n",
       "1         0.481619     0.585617     0.534069     0.407364     0.497220  \n",
       "2         0.330888     0.517985     0.472785     0.414247     0.492908  \n",
       "3         0.433361     0.590519     0.584408     0.413019     0.422263  \n",
       "4         0.468250     0.671773     0.464809     0.433602     0.542738  \n",
       "...            ...          ...          ...          ...          ...  \n",
       "41994     0.415228     0.586375     0.539669     0.364866     0.246553  \n",
       "41995     0.521826     0.610126     0.510385     0.407463     0.431950  \n",
       "41996     0.487935     0.714094     0.343001     0.379957     0.417166  \n",
       "41997     0.572347     0.546865     0.463921     0.352464     0.433093  \n",
       "41998     0.532193     0.659391     0.541830     0.436454     0.546414  \n",
       "\n",
       "[41999 rows x 61 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_features42k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_removal(df, alpha=0.05):\n",
    "    # x: feature vector\n",
    "    # df: pd.DataFrame.values -> dataset data\n",
    "    # alpha: sensitivity for outliers\n",
    "    n = df.values.shape[0]\n",
    "    p = df.values.shape[1]\n",
    "\n",
    "    mu = np.mean(df.values, axis=0, keepdims=True)\n",
    "    #rowvar change since observation are in the rows\n",
    "    cov = np.cov(df.values, rowvar=False) \n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    # compute the squared malanobis distance, cap the minimum to only positive values\n",
    "    malanobis_dist = np.sqrt(np.maximum((df.values - mu) @ inv_cov @ (df.values - mu).T,0)).diagonal()\n",
    "\n",
    "    threshold = chi2.ppf((1 - alpha), p)\n",
    "    outliers = malanobis_dist > np.sqrt(threshold)\n",
    "    outliers_removed = df.values[~outliers]\n",
    "    return pd.DataFrame(data=outliers_removed, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_iris_df = normalize_min_max(iris_df.iloc[:, :4])\n",
    "iris_classes_df = iris_df.iloc[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_iris = outlier_removal(norm_iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier_feature42k = outlier_removal(norm_features42k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.posterior = None\n",
    "        self.mean = None\n",
    "        self.var = None\n",
    "        self.prior = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        N, M = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        num_classes = len(self.classes)\n",
    "\n",
    "        self.mean = np.zeros((N,M))\n",
    "        self.var = np.zeros((N,M))\n",
    "        self.prior = np.zeros(N)\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            # slice the input to only\n",
    "            X_class = X[y==c].values\n",
    "            self.mean[idx, :] = X_class.mean(axis=0)\n",
    "            self.var[idx, :] = X_class.var(axis=0)\n",
    "            N_class = X_class.shape[0]\n",
    "            self.prior[idx] = N_class / N\n",
    "    \n",
    "    def __gaussian(self, class_idx, xi):\n",
    "        mean = self.mean[class_idx]\n",
    "        var = self.var[class_idx]\n",
    "        gaussian = np.exp(-(xi-mean) ** 2 / (2 * var)) / np.sqrt(2 * np.pi * var)\n",
    "        return gaussian\n",
    "\n",
    "    def predict(self, X):\n",
    "        N = X.shape[0]\n",
    "        num_classes = len(self.classes)\n",
    "        self.posterior = np.zeros((N, num_classes))\n",
    "\n",
    "        for i, xi in enumerate(X.values):\n",
    "            class_posteriors = np.zeros(num_classes)\n",
    "            for j, c in enumerate(self.classes):\n",
    "                prior = self.prior[j]\n",
    "                likelihood = 1\n",
    "                for idx in range(len(xi)):\n",
    "                    likelihood *= self.__gaussian(j, xi)[idx]\n",
    "                class_posteriors[j] = prior * likelihood\n",
    "\n",
    "            total_posterior = np.sum(class_posteriors)\n",
    "            self.posterior[i, :] = class_posteriors / total_posterior\n",
    "\n",
    "        return self.classes[np.argmax(self.posterior, axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "X = norm_iris_df\n",
    "y = pd.Series(iris_classes_df)\n",
    "\n",
    "species_to_label = {\"setosa\" : 0, \"versicolor\" : 1, \"virginica\" : 2}\n",
    "y = y.map(species_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:  0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "    \n",
    "    bc = BayesClassifier()\n",
    "    bc.train(X=X_train, y=y_train)\n",
    "\n",
    "    y_pred = bc.predict(X_test)\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, y_pred=y_pred))\n",
    "\n",
    "print(\"Average accuracy: \", np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Parzen Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParzenWindow:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __gaussian_kernel(self, x, xi, sigma):\n",
    "        D = x.shape[0]\n",
    "        return (1 / (np.sqrt(2 * np.pi) * sigma) ** D) * np.exp(-np.linalg.norm(x - xi) ** 2 / (2 * sigma ** 2))\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        self.X = X.values\n",
    "        self.y = y.values.flatten()\n",
    "        self.classes = np.unique(self.y)\n",
    "        self.prior = [np.mean(self.y == c) for c in self.classes]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X.values:\n",
    "            probabilities = []\n",
    "            for j, c in enumerate(self.classes):\n",
    "                class_samples = self.X[self.y == c]\n",
    "                prob_window = np.mean([self.__gaussian_kernel(x, xi, self.sigma) for xi in class_samples])\n",
    "                probabilities.append(prob_window * self.prior[j])\n",
    "            predictions.append(self.classes[np.argmax(probabilities)])\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = norm_features42k.iloc[:, 1:]\n",
    "#y = norm_features42k.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:  0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "parzen_accuracies = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "    \n",
    "    pw = ParzenWindow(sigma=0.5)\n",
    "    pw.train(X=X_train, y=y_train)\n",
    "\n",
    "    y_pred = pw.predict(X_test)\n",
    "\n",
    "    parzen_accuracies.append(np.mean(y_pred == y_test.values.flatten()))\n",
    "\n",
    "print(\"Average accuracy: \", np.mean(parzen_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, kernel=\"linear\", sigma=0.5, tolerance=1e-4, max_iter=1000):\n",
    "        self.kernel_name = kernel\n",
    "        self.sigma = sigma\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = None\n",
    "        self.support_vectors = None\n",
    "        self.y = None\n",
    "        self.X = None\n",
    "        self.b = None\n",
    "    def _gaussian_kernel(self, x, xi):\n",
    "        D = x.shape[0]\n",
    "        return (1 / (np.sqrt(2 * np.pi) * self.sigma) ** D) * np.exp(-np.linalg.norm(x - xi) ** 2 / (2 * self.sigma ** 2))\n",
    "\n",
    "    def kernel(self, xi, xj):\n",
    "        if self.kernel_name == \"linear\":\n",
    "            return np.dot(xi, xj.T)\n",
    "\n",
    "        elif self.kernel_name == \"rbf\":\n",
    "            K = np.zeros((xi.shape[0], xj.shape[0]))\n",
    "            for i in range(xi.shape[0]):\n",
    "                for j in range(xj.shape[0]):\n",
    "                    K[i, j] = self._gaussian_kernel(xi[i], xj[j])\n",
    "            return K \n",
    "\n",
    "    def train(self, X, y):\n",
    "        X = X.values\n",
    "        y = y.values\n",
    "        \n",
    "        N, M = X.shape\n",
    "        K = self.kernel(X, X)\n",
    "        \n",
    "        self.alpha = np.zeros(N)\n",
    "        \n",
    "        lr = 0.001 \n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            gradient = np.ones(N)\n",
    "            for i in range(N):\n",
    "                sum_term = 0\n",
    "                for j in range(N):\n",
    "                    sum_term += self.alpha[j] * y[j] * K[i, j]\n",
    "                gradient[i] -= y[i] * sum_term\n",
    "                \n",
    "            self.alpha += lr * gradient\n",
    "            \n",
    "            if np.linalg.norm(gradient) < self.tolerance:\n",
    "                break\n",
    "        \n",
    "        idx = self.alpha > self.tolerance\n",
    "        self.X = X[idx]\n",
    "        self.support_vectors = self.X\n",
    "        self.y = y[idx]\n",
    "        self.alpha = self.alpha[idx]\n",
    "        \n",
    "        b_sum = 0\n",
    "        for i in range(len(self.alpha)):\n",
    "            sum_term = 0\n",
    "            for j in range(len(self.alpha)):\n",
    "                sum_term += self.alpha[j] * self.y[j] * self.kernel(self.X[i:i+1], self.X[j:j+1])[0, 0]\n",
    "            b_sum += self.y[i] - sum_term\n",
    "        self.b = b_sum / len(self.alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.values\n",
    "        K = self.kernel(X, self.support_vectors)\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            sum_term = 0\n",
    "            for j in range(len(self.alpha)):\n",
    "                sum_term += self.alpha[j] * self.y[j] * K[i, j]\n",
    "            predictions[i] = sum_term + self.b\n",
    "\n",
    "        return np.sign(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:  0.4333333333333334\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "svm_accuracies = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "\n",
    "    svm = SVM(tolerance=1e-4, max_iter=1000)\n",
    "    svm.train(X_train, y_train)\n",
    "\n",
    "    predictions = svm.predict(X_test)\n",
    "    svm_accuracies.append(np.mean(predictions == y_test.values))\n",
    "\n",
    "print(\"Average accuracy: \", np.mean(svm_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:  0.4333333333333334\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "svm_rbf_accuracies = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "\n",
    "    svm = SVM(kernel=\"rbf\", sigma=0.5, tolerance=1e-4, max_iter=1000)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    predictions = svm.predict(X_test)\n",
    "    svm_rbf_accuracies.append(np.mean(predictions == y_test.values))\n",
    "\n",
    "print(\"Average accuracy: \", np.mean(svm_rbf_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Best Results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose your difficulty: \n",
      "Starting a new game of tic-tac-toe.\n",
      "Spaces are numbered as follows:\n",
      "0 1 2\n",
      "3 4 5\n",
      "6 7 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run tictactoe/tic_tac_toe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
